\documentclass[a4paper, 12pt]{report}
\usepackage[UTF8]{ctex}
\usepackage{graphicx} % for pictures
\usepackage[margin=2cm]{geometry}
\usepackage{tocloft} % for tableofcontents
\usepackage{float} % for pictures
\usepackage{xeCJK}% for chinese input
\usepackage{setspace} % for line spacing
\usepackage{anyfontsize}% set font size
\usepackage{titlesec} % set chapter style
\usepackage{CJKnumb} % for chinese number
\usepackage{titletoc} % title in TOC
\usepackage{amsmath} % for mathematical expressions, such as matrix
\usepackage{caption} % captipon setting
\usepackage{multirow} % for multirow in tables
\usepackage{hyperref} % for hyperlink
\usepackage[nottoc]{tocbibind} % add bibliography to toc



%%% font settings
\setCJKmainfont{SimSun}
\setmainfont{Times New Roman}
%\newCJKfontfamily[mingliu]\mlu{MingLiU}

\captionsetup{labelsep=space} % remove colon in caption
\captionsetup[table]{labelsep=space}

%%% format settings
% 要注意，latex的字号变动会覆盖baselineskip
% latex默认的baselineskip是fontsize的1.2倍，所以改了fontsize之后baselineskip也变了
% 行距计算：baselineskip * baselinestretch，baselinestretch和linespread其实是差不多的，只是取值不一样，后者取1.3对应前者的1.5，1.6对应2；所以一般设置行距直接改后面的因子

%%% new commands
\newcommand{\mainheader}{东南大学 2019 届本科生毕业设计（论文）}
\renewcommand{\contentsname}{\centerline{ \fontsize{16}{19.2} \selectfont \heiti 目 \qquad 录 } }
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\cftdot}{$\cdot$}
\renewcommand{\cftdotsep}{1}
\setlength{\cftbeforechapskip}{12pt}
\newcommand{\acknowledgementtitle}{\heiti \fontsize{14}{17} \selectfont 致\quad 谢}
\newcommand{\acknowledgementtitletoc}{致谢}
%\newcommand{\referencetitle}{\protect\leftline{\heiti \fontsize{14}{17} \selectfont 参考文献}}
%\newcommand{\referencetitletoc}{参考文献}
\renewcommand{\arraystretch}{1.5} % table stretch
\renewcommand{\labelenumi}{\roman{enumi}) } % change enumeratior index
%\setcounter{secnumdepth}{3}% Number \subsubsection https://tex.stackexchange.com/questions/146304/subsubsection-in-a-report-style-document/146307 Very good information about the toc and numbering for reports
%\setcounter{tocdepth}{3}
%\renewcommand{\bibname}{\protect\leftline{\heiti \fontsize{14}{17} \selectfont 参考文献}}
\renewcommand{\bibname}{\protect\leftline{参考文献}}


%%% 中文摘要
\renewenvironment{abstract}[1]
{
\newcommand{\keywords}{#1}
\phantomsection
\addcontentsline{toc}{chapter}{摘要\quad }

\vspace*{12pt}
\begin{center}
\fontsize{18}{21.6}\selectfont 摘\qquad 要
\end{center}
}
{
  \\[1\baselineskip]
  关键词： \keywords
}

%%% 英文摘要
\newenvironment{englishabstract}[1]
{
\newcommand{\keywords}{#1}
\phantomsection
\addcontentsline{toc}{chapter}{Abstract\quad }

\vspace*{12pt}
\begin{center}
ABSTRACT
\end{center}
}
{
\\[1\baselineskip]
KEY WORDS: \keywords
}

%%% 章节格式
\titleformat
{\chapter} % command
{\centering \heiti \fontsize{15}{18} \selectfont} % format
{第\CJKnumber{\thechapter}章} % label
{0.1ex} % sep
{
   \centering
} % before-code

%%% section格式
\titleformat
{\section} % command
{\heiti \fontsize{14}{17} \selectfont} % format
{\thesection \ } % label
{0.1ex} % sep
{} % before-code

%%% subsection格式
\titleformat
{\subsection} % command
{\fontsize{12}{14} \selectfont \bfseries} % format
{\thesubsection \ } % label
{0.1ex} % sep
{
\bfseries
} % before-code


%%% 目录中章节
\newcommand{\titlecontentschapter}{%
\titlecontents{chapter}[0pt]{\vspace{.5\baselineskip}\normalfont}
{第\CJKnumber{\thecontentslabel}章\quad}{}
{\hspace{.5em}\titlerule*[4pt]{$\cdot$}\contentspage}
}

%%% 致谢
\newenvironment{Acknowledgement}[1][\acknowledgementtitle]
{%
  
  \phantomsection
  \addcontentsline{toc}{chapter}{\acknowledgementtitletoc}
  \chapter*{#1}

}

%%% 参考文献 没用上
\newenvironment{References}[1][\referencetitle]
{%
  
  \phantomsection
  \addcontentsline{toc}{chapter}{\referencetitletoc}
  \chapter*{#1}

}








\begin{document}

%%% 封面
\vspace*{36pt}
\thispagestyle{empty}
\begin{figure}[H]
\centering
\includegraphics[width=4.28in, height = 1.52in]{seu}
\end{figure}
\vspace{16pt}

\begin{center}
{\fontsize{16}{46}\selectfont % first : fontsize; second : baselineskip

\textbf{题目} \quad \underline{\makebox[250pt][c]{\textbf{图像去雾化方法研究}}}\\
\underline{\makebox[100pt][c]{\textbf{交通学院}}}院（系）\quad \underline{\makebox[100pt][c]{\bf 交通工程}}专业 \\
学\qquad 号 \underline{\makebox[300pt][c]{\textbf{21015111}}}\\
学生姓名 \underline{\makebox[300pt][c]{\textbf{周冬秦}}}\\
指导教师 \underline{\makebox[300pt][c]{\textbf{何铁军\  教授}}}\\
起止日期 \underline{\makebox[300pt][c]{\textbf{2019年2月25日至2019年6月2日}}}\\
设计地点 \underline{\makebox[300pt][c]{\textbf{东南大学交通学院203室}}}\\
}
\end{center}
\newpage

%%% 独创性声明、授权声明
\thispagestyle{empty}
\vspace*{36pt}
\begin{center}
\textbf{\fontsize{16}{19.2} \selectfont 东南大学毕业（设计）论文 \\ \medskip 独创性声明}
\end{center}
\par 本人声明所呈交的毕业（设计）论文是我个人在导师指导下进行的研究工作及取得的研究成果。尽我所知，除了文中特别加以标注和致谢的地方外，论文中不包含其他人已经发表或撰写过的研究成果，也不包含为获得东南大学或其它教育机构的学位或证书而使用过的材料。与我一同工作的同志对本研究所做的任何贡献均已在论文中作了明确的说明并表示了谢意。
\begin{flushright}
论文作者签名：\rule{2cm}{0.05em} \quad 日期：\rule{2cm}{0.05em}
\end{flushright}
\vspace{4cm}

\begin{center}
\textbf{\fontsize{16}{19.2} \selectfont 东南大学毕业（设计）论文使用 \\ \medskip 授权声明}
\end{center}
\par 东南大学有权保留本人所送交毕业（设计）论文的复印件和电子文档，可以采用影印、缩印或其他复制手段保存论文。本人电子文档的内容和纸质论文的内容相一致。除在保密期内的保密论文和在技术保护期限内的论文外，允许论文被查阅和借阅，可以公布（包括以电子信息形式刊登）论文的全部或中、英文摘要等部分内容。论文的公布（包括以电子信息形式刊登）授权东南大学教务处办理。
\begin{flushright}
论文作者签名：\rule{2cm}{0.05em} \quad 导师签名：\rule{2cm}{0.05em} \quad 日期：\rule{2cm}{0.05em}
\end{flushright}
\newpage

%%% 摘要
\pagenumbering{Roman}
\begin{abstract}{暗通道法，DehazeNet，MSCNN，AOD-Net，图像质量评估}
{\setstretch{1.5}
本文研究了图像去雾领域的标志性算法。
}
\end{abstract}

\newpage

\begin{englishabstract}{dark channel prior, DehazeNet, MSCNN, AOD-Net, image quality assessment}
\par In this paper, we ...
\end{englishabstract}
\newpage

%%% 目录
\begingroup
\renewcommand{\leftline}{}
\renewcommand{\bibname}{}

{\fontsize{12pt}{18pt} \selectfont
\tableofcontents
}\thispagestyle{empty}
\newpage
\endgroup

%%% 正文
\pagenumbering{arabic}
\titlecontentschapter   % 在这里才启用前面定义的目录章节格式，不在前面启用，因为摘要也算作章节，牛逼！
\chapter{绪论\quad}
\section{研究背景和意义\quad}
信息化时代的一大特征即是海量数据的存储，例如图片、视频、音频、文本数据等。以图片数据为例，绝大部分的图片都采集于室外，但室外采集的图片相比于室内图片，更容易受到空中浑浊物，如水滴、悬浮颗粒的降质影响。降质图片无法反映原景物的对比度和真实颜色，降低观赏性，某些图片还会丢失重要信息，如交叉口抓拍图片。

图像去雾化处理是一个热门的研究领域。一方面，图像去雾能够有助于高层次的计算机视觉研究，如目标检测、无人驾驶等；另一方面，经过去雾处理的图片能够较好地纠正色差，更具观赏性。此外，在去雾处理中生成的深度图也会有助于图像编辑和其他视觉算法\cite{ref1}。

含雾图片形成可以由大气散射模型来描述，这一模型最初是由McCartney\cite{ref2}提出来的，之后由Narasimhan和Nayar\cite{ref3, ref4}进一步发展。模型表达式如下：
\begin{equation}
{\bf I(x)} = {\bf J(x)}t{\bf (x)} + {\bf A}(1 - t{\bf (x)})
\end{equation}
其中，$I$是获取的含雾图片，$J$是对应的无雾图片，$t$是介质散射率，$A$是大气光值。去雾的目的即为，根据已有的$I$，获取对应的$J$，$A$，$t$。若介质时均质的，介质散射率可以表示为：
\begin{equation}
t{\bf (x)} = e^{-\beta d{\bf (x)}}
\end{equation}
其中，$\beta$是大气散射系数，为未知值，$d$为景深。

\section{研究现状\quad}
由大气散射模型可知，图像去雾的关键步骤在于估计出透射图和大气光。含雾图片的形成及其逆过程图像去雾都与景深有关。对于一张随意采集的图像，我们无法获取其景深，因此单幅图片的去雾是一个约束不足的问题。

针对这一困境，许多学者提出使用多幅图像获取附加信息进行去雾\cite{ref3, ref5, ref6, ref7, ref8}，但这些方法的应用场景会受到限制。以自动驾驶为例，行驶中的车辆无法对同一处景物进行多次拍照，而多个摄像头同时拍照则无法保证拍照取景完全一致，难以获取同一景物的多幅图像；此外，此类算法需要对多幅图片进行处理，速率较慢，而自动驾驶车辆获取的数据量巨大，如特斯拉自动驾驶汽车8个摄像头每秒有2100帧输入图像，多幅图像去雾无法满足其处理速度要求。

理想的去雾算法可以对摄像头拍摄的图片进行实时去雾，因此单幅图片的高效、快速去雾成为研究的热点。目前，单幅图像去雾主要有两类较高效的方法：基于先验知识的去雾方法和基于神经网络的去雾方法，这两类方法都基于上述的大气散射模型。除此之外，还有不基于物理模型的图像增强方法，如直方图均衡化算法\cite{ref9}、Retinex算法\cite{ref10}等，但此类方法不考虑有雾图像的生成原因，直接对关注的细节进行增强，虽然简便易行，但容易丢失图像信息，使图像失真，本论文不对此类算法进行深入研究。

\subsection{基于先验知识的去雾算法\quad}
Tan\cite{ref11}等人观察到无雾图像比其对应有雾图像的对比度高，于是采用了最大化对比度的方法来去除图像中的雾，该方法在视觉上能取得一定的效果，但是容易使图像过饱和及颜色失真。Fattal\cite{ref12}假设透射率局部不相关、反射率局部为常数，估计出景物的反射率并推导出透射率，进而计算出原图像，这一方法假设太强并且未考虑到景物深度的结构，导致无法处理浓雾图像并且不能准确估计景物深度。He\cite{ref1}等人在统计分析大量无雾图像后，于2009年提出了基于暗通道先验的去雾算法。这一方法非常简单，其去雾效果却非常好。He最初提出该方法时采用了软抠图的方法来优化透射率，但是该方法计算效率太低，其在2013年提出的引导滤波\cite{ref13}方法可以大幅度提升计算效率。在He之后，有许多学者研究如何改进He的算法以使其适用于天空区域，并尝试优化透射率的估计以及计算效率。这一方法主要问题是对于含天空区域的图像，暗通道会失效，天空区域进行去雾处理后会失真。Zhu\cite{ref14}等人提出了基于颜色衰减先验的去雾方法，该方法提出了一个线性模型来表示颜色衰减先验下含雾图像中的景物深度，之后采用监督学习的方法学习到模型参数，从而获取图像中的深度信息，并以此来估计透射图从而给图像去雾。Berman\cite{ref15}提出了基于非局部先验的去雾方法，该方法假设无雾图像中每个颜色团簇都是RGB空间中的一条雾线。

\subsection{基于神经网络的去雾算法\quad}
随着卷积神经网络的兴起及其在计算机视觉领域的大规模应用，一些去雾算法也开始应用卷积神经网络。Cai\cite{ref16}等人提出了一个可训练的系统DehazeNet，用以估计含雾块的透射率，而后将此系统应用到整张图片以获取含雾图片的透射图，从而根据大气散射模型计算出无雾图片。Ren\cite{ref17}等人提出了一个多尺度卷积神经网络 (MSCNN) 用以直接估计与含雾图片对应的透射图，该方法先使用粗尺度模型对含雾图像进行处理，之后将此模型输出和原含雾图像作为细尺度模型的输入，而后使用细尺度模型输出的透射图进行无雾图像的计算。Li\cite{ref18}等人提出了一个端到端的去雾网络AOD-Net，该方法先将透射率$t$和大气光$A$重建为一个新的变量，而后使用两个模块构成的AOD-Net对含雾图像进行端到端的处理。

\section{研究目的和研究内容\quad}
本论文旨在探索图像去雾化的研究前沿，实现去雾算法，并对其效果进行对比。此外，本论文将去雾算法应用到雾天视频，以提供更好的路面交通信息。

本论文主要内容是实现去雾领域较为代表性的算法，例如暗通道法（DCP），DehazeNet, MSCNN, AOD-Net。第一章介绍去雾研究的背景意义，去雾研究的现状综述。第二章以暗通道法为例介绍基于先验知识的图像去雾算法，第三章以DehazeNet, MSCNN, AOD-Net为主要内容介绍基于神经网络的去雾算法。第四章介绍领域内广泛使用的数据集，以及上述各算法具体实现和其性能评估。第五章总结全文并提出后续研究方向和本研究应用前景展望。


\chapter{基于先验知识的去雾算法研究\quad}
本章以暗通道法为例介绍基于先验知识的去雾算法，并对该算法进行描述性的评价。先验知识为不对问题进行具体处理即可知道的知识，即先于经验之前的知识。与之对应的是后验知识，即在有经验之后的知识。去雾处理中的先验知识，即不对图像进行去雾，只凭观察对比含雾图像或去雾图像就能获得的知识。在应用神经网络的去雾方法变得较为流行之前，单幅图像的去雾方法主要是基于不同的先验知识，如本章介绍的暗通道先验。
\section{暗通道\quad }
暗通道是He[1]等人观察分析大量无雾图片所得出的。他们发现，大多数户外无天空区域的图片块中存在某些像素点，这些像素点有至少一个很小的RGB分量值，也就是说这些图片块中最小的像素值很小，甚至接近于0。这一发现可由下式表示：
\begin{equation}
{\bf J}^{dark}{\bf (x)} = \min_{c \in \{r, g, b\}}(\min_{{\bf y} \in \Omega({\bf x})}(\bf J^c(y)))
\end{equation}
其中，$J$是一张户外无天空不含雾图片，$J^c$是其中一个颜色通道，$\Omega(x)$是以x为中心的一个图片块。他们观察到$J^{dark}$非常小，且接近于0，于是将$J^{dark}$称为图片J的暗通道，而将式(2.1)称为暗通道先验。

	暗通道的发现也有其物理背景。He等人提出，户外图片常常具有很多阴影，并且是具有多种颜色的，而阴影的存在会使得各通道像素值较小，具有鲜明颜色的物体则往往会缺少某一通道的颜色，使得该通道像素值较小，因此户外无天空区域图像的暗通道确实应该是暗的。理论分析之外，他们从网上搜集了5000张无雾风景图，裁剪掉天空区域并统计了他们的暗通道像素强度，统计数据也支持了他们提出的暗通道理论。



\section{透射图估计\quad }
估计透射图之前，He等人先假设大气光值$A$已给定且为常数，并且假设以$x$为中心的局部区域内透射率为常数，记之为$\tilde{t}(x)$，对大气散射模型(1.1)两端取最小值则有
\begin{equation}
\min_{{\bf y} \in \Omega({\bf x})}({\bf I}^c({\bf y})) = \tilde{t}(x) \min_{{\bf y} \in \Omega({\bf x})}({\bf J}^c({\bf y})) + (1-\tilde{t}(x)){\bf A}^c
\end{equation}
两端同时除以大气光值有
\begin{equation}
\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf I}^c({\bf y})}{{\bf A}^c}) = \tilde{t}(x)\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf J}^c({\bf y})}{{\bf A}^c}) + (1-\tilde{t}(x))
\end{equation}
在三个通道之内取最小值有
\begin{equation}
\min_c(\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf I}^c({\bf y})}{{\bf A}^c})) = \tilde{t}(x)\min_c(\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf J}^c({\bf y})}{{\bf A}^c})) + (1-\tilde{t}(x))
\end{equation}
根据暗通道先验有
\begin{equation}
{\bf J}^{dark}{\bf (x)} = \min_c(\min_{{\bf y} \in \Omega({\bf x})}(\bf J^c(y))) = 0
\end{equation}
而大气光值$A^c$恒为正值，则有
\begin{equation}
\min_c(\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf J}^c({\bf y})}{{\bf A}^c})) = 0
\end{equation}
联式(2.4)和(2.6)有
\begin{equation}
\tilde{t}(x) = 1 - \min_c(\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf I}^c({\bf y})}{{\bf A}^c}))
\end{equation}

在含雾图片中，天空区域的颜色和大气光值很相近，因此由(2.7)计算出天空区域透射率为
$$\min_c(\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf I}^c({\bf y})}{{\bf A}^c})) \rightarrow 1,\ \tilde{t}(x) \rightarrow 0$$
而天空是在无限远处的，透射率应当为0，这一结果与(2.7)式计算结果一致，因此计算透射率时无需区分开天空区域和非天空区域。

在实际中，即便是无雾天气空中也会存在小颗粒，在远眺时图像仍会被降质。而倘使将小颗粒全部去除，即令$\tilde{t}(x) = 0$，则人眼无法感知景物深度，使看到的景物不真实，因此可在(2.7)中引入一个常数$\omega$以保留空中的部分颗粒，结果如下
\begin{equation}
\tilde{t}(x) = 1 - \omega\min_c(\min_{{\bf y} \in \Omega({\bf x})}(\frac{{\bf I}^c({\bf y})}{{\bf A}^c}))
\end{equation}
其中，$\omega$取值为0.95。

\section{引导滤波\quad}
He等人初始提出暗通道法时，采用的是软抠图方法对(2.8)计算出的透射图进行优化，但该方法计算效率太低，难以应对快速的去雾处理。其在2013年提出的引导滤波\cite{ref13}方法能够较好地提升计算效率，同时保持了暗通道法较好的去雾效果。

\subsection{滤波\quad}
一张图片可以视作一个函数，自变量为图片中像素点的位置，因变量为像素值的大小，由此我们可以得到与每张图片唯一对应的一个函数。如是灰度图片，自变量为二元坐标值$(x, y)$，因变量$f(x, y)$则为处于0-255之间的一个整数值；如是彩色图片，自变量仍为二元坐标值$(x, y)$，因变量则为$3\times 1$的向量，该向量分量值为每个颜色通道的像素值（0-255之间的整数）
$$ f(x. y) = 
\begin{vmatrix}
  r(x, y) \\
  g(x, y) \\
  b(x, y)  
\end{vmatrix}
$$

滤波的作用即改变像素点位置对应的像素值，即$f(x,y)$，但不改变像素点的位置$(x,y)$，是常用的图像增强、提取图像信息的手段。常用滤波器有线性时不变滤波器，如均值滤波、中值滤波、高斯滤波，这类滤波器有显式声明的核函数，不因图片内容产生变化。例如，均值滤波会用每个像素点周围窗口内像素值的平均值取代该像素点原值：
$$g(x) = \frac{1}{|\Omega(x)|}\sum_{y \in \Omega(x)}f(y)$$
式中$\Omega(x)$为以$x$为中心的窗口大小，该大小可自定义，$|\Omega(x)|$为该窗口内像素点的个数。图2.1是含有高斯噪声的图片，我们使用均值滤波对该图片去噪，窗口大小选择为$7\times 7$，去噪效果如图2.2所示。
\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{noisy}
\caption{含噪图片}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{de-noisy}
\caption{去噪图片}
\end{minipage}
\end{figure}
均值滤波算法简单，对图像处理速度较快，但在降低噪声的同时容易使图像变模糊，尤其是图像边缘部分，这是因为滤波在处理边缘部分时会进行补零操作，进行补零操作后的像素点不具备原来图像的信息，因而造成模糊。此外，均值滤波较适合高斯噪声的去噪，对椒盐噪声处理效果较差。除线性时不变滤波器之外，有时候我们想利用图片的信息进行对图片进行滤波处理，因而需要使用一张引导图像，即引导滤波。

\subsection{引导滤波\quad}
引导滤波的输入为引导图像$I$，被引导图像$p$，滤波器半径大小$r$，正则化参数$\epsilon$，输出为滤波处理后的图像$q$。引导滤波的关键假设是引导图像$I$与滤波输出$q$之间存在线性关系，在以像素点$k$为中心的窗口$\omega_k$中，滤波输出是引导图像的线性转换
\begin{equation}
q_i = a_k I_i + b_k, \forall i \in \omega_k
\end{equation}
其中，$(a_k, b_k)$是在窗口$\omega_k$中为常数的线性系数。此滤波器使用以$r$为半径大小的窗口。

我们通过最小化滤波器输入$p$与输出$q$之间的差异来确定线性系数的大小，由此定义的损失函数为
\begin{equation}
E(a_k, b_k) = \sum_{i\in \omega_k}((a_k I_i + b_k - p_i) + \epsilon a_k^2)
\end{equation}
其中$\epsilon$是正则化系数，用以惩罚过大的系数$a_k$。最小化上述损失函数，得
\begin{equation}
a_k = \frac{\frac{1}{|\omega|}\sum_{i\in \omega_k}I_ip_i - \mu_k\bar{p_k}}{\sigma_k^2 + \epsilon}
\end{equation}

\begin{equation}
b_k = \bar{p_k} - a_k \mu_k
\end{equation}
其中，$\mu_k$和$\sigma_k^2$是引导图像在窗口$\omega_k$中的均值和方差，$|\omega|$是窗口内像素点的个数，而$\bar{p_k} = \frac{1}{|\omega|}\sum_{i\in \omega_k}p_i$是窗口内被引导图像的均值。得到线性系数$(a_k, b_k)$值后，我们即可通过(2.9)式计算滤波器输出。

值得注意的是，图片中某个像素点$i$不为某一个窗口专属，因此在不同窗口下根据(2.9)式计算出的$q_i$并不一样。对此，一个较为简单可行的方法是对$q_i$所有可能值取平均。因此，在计算出每个窗口中的线性系数之后，我们可以通过下式计算滤波器输出
\begin{equation}
q_i = \frac{1}{|\omega|}\sum_{k|i\in \omega_k}(a_k I_i + b_k)
\end{equation}
也即
\begin{equation}
q_i = \bar{a_i}I_i + \bar{b_i}
\end{equation}
其中，$\bar{a_i} = \frac{1}{|\omega|}\sum_{k\in \omega_i}a_k$和$\bar{b_i} = \frac{1}{|\omega|}\sum_{k\in \omega_i}b_k$是包含像素点$i$的所有窗口中线性系数的均值。式(2.11), (2.12), (2.14)即是引导滤波的定义。由以上介绍的算法，我们可以对2.2节中估计出的透射图进行优化，以此得到更精确的透射图。

\section{大气光估计\quad}
He等人提出的{\Large 暗通道可用以估计图片中雾的厚度}：暗通道中亮度值越高，表明原图中该区域雾越厚。由此，He等人提出，先选取暗通道中前0.1\%亮度值最高的像素点，在这些像素点中选取原图中的最高亮度值作为大气光$A$。

\section{无雾图像计算\quad}
如前所述，根据大气散射模型(1.1)进行图像去雾处理，最关键步骤是计算出透射图和大气光。值得注意的是，在介质透射率较小甚至趋近于0时，根据模型获取的无雾图像易产生较大噪声，因此需要对透射率最小值进行限制，可由下式进行无雾图像的计算
\begin{equation}
{\bf J(x)} = \frac{\bf I(x) - A}{max(t({\bf x}), t_0)} + {\bf A}
\end{equation}
其中，$t_0$取值0.1。

\section{算法评价\quad}
He等人提出的暗通道法毫无疑问是单幅图像去雾的标志性工作。该方法原理非常简单，但是去雾效果非常好。在暗通道法提出后，许多学者对此方法表现出极大兴趣，并致力于改进该算法，以使其更好的适用于天空区域。

与此同时，我们也应当意识到这一方法的局限性，其一，是多次提到过的，该方法不适用于存在和天空颜色较为相似的物体的图片，因为在此类图片中，暗通道会失效。其二，是所有基于大气散射模型的去雾算法共有的局限性——该大气散射模型也许无法准确描述含雾图片的生成。

\chapter{基于神经网络的去雾算法研究\quad}
本章着重介绍几种广为人知的基于神经网络的去雾算法，DehazeNet, MSCNN, AOD-Net。神经网络以其强大的非线性拟合能力在学界掀起了一股热潮，而卷积神经网络以其对图像的卓越处理能力成为了计算机视觉领域的标志性算法。本章将先介绍卷积神经网络的基本结构，如卷积层、池化层等，再深入讲解以卷积神经网络为基础的去雾算法。

\section{卷积神经网络\quad}
卷积神经网络最早是由Yang LeCun提出的，但当时并未引起较大注意。之后的十几年，计算能力显著提高，CPU越来越快，GPU成为常用的计算工具，智能手机、摄像头的广泛应用使人们可以获取海量数据，大量的数据和不断提高的计算能力让神经网络的训练变得越来越易行。2012年由Alex Krizhevsky提出的AlexNet在ImageNet图像识别比赛中大幅获胜，激起了卷积神经网络的热潮，并由此拉开了深度学习的时代序幕。自那时起，卷积神经网络便成为了深度学习的一把利器。

典型的卷积神经网络一般含有卷积层、池化层和全连接层。

\subsection{卷积层\quad}
卷积操作和上述线性时不变滤波器对图片进行的操作是类似的，{\Large 如图3.1所示}，其中$I$为需进行卷积操作的图层，$K$为卷积核，卷积结果为$I\ast K$（也常记为$I\otimes K$）。卷积层一般用于提取前一层的局部特征，卷积核即特征提取器。
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{convolution}
\caption{卷积操作}
\end{figure}
以彩色图片为输入层时，其大小为$M \times N \times D$，其中$M$为图片高度，$N$为图片宽度，$D$为图片深度，即颜色通道个数，彩色图片通道个数为3（$D = 3$）。不失一般性，不妨假设输入卷积层的张量大小为$M \times N \times D$，记为$X$，进行卷积操作后输出层张量大小为 $M^{\prime} \times N^{\prime} \times P$，记为$Y$，则卷积核张量大小为$m \times n \times D \times P$，记为$W$，其中，$m$，$n$为二维卷积核大小，$P$为输出特征图的个数。

卷积层计算输出特征图$Y_p$时，用卷积核$W_p$（大小为$m \times n \times D$）对输入张量$X$进行卷积操作，加上偏置标量$b_p$即得到卷积层的净输入$Z_p$，$Z_p$经过激活函数得到特征图$Y_p$。其计算过程如下
\begin{equation}
Z^p = W^p \otimes X + b^p = \sum_{d = 1}^D W^{p, d} \otimes X^d + b^p
\end{equation}

\begin{equation}
Y^p = f(Z^p)
\end{equation}
其中，$f$为激活函数，常用的激活函数有tanh, ReLU, sigmoid, Leaky ReLU。一个卷积层需要$P \times D \times (m \times n) + P$个参数。

\subsection{池化层\quad}
池化层，又叫下采样层，一般置于卷积层之后，目的是进行特征选择，减少特征数量。常用的池化函数有最大池化和平均池化，两者作用原理如图3.2所示。
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{pool}
\caption{池化操作}
\end{figure}
池化层的一般操作是将每张特征图划分为$2\times 2$的不重叠区域，对每个区域使用最大池化或平均池化函数。其中，划分区域及池化函数可以自定义，但过大的划分区域会导致信息的过分损失。

\subsection{上采样层\quad}
上采样层与池化层对应，类似于池化层的逆操作，即将一个像素点的值扩充至以该像素点为中心的窗口，窗口大小可以自己定义，一般选择$2\times 2$的窗口。其过程如图3.3所示，此图选择的窗口大小为 $3 \times 3$。
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{upsample}
\caption{上采样操作}
\end{figure}

\section{DehazeNet\quad}
由大气散射模型可知，图片去雾最关键步骤在于透射图的估计，由是Cai\cite{ref16}等人提出了一个端到端预测透射率的系统，该网络架构如图3.4所示。值得注意的是，该网络输入层是大小为$16 \times 16 \times 3$的图片块，输出结果是该图片块的透射率，这一性质将在第四章详细讨论。
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{DehazeNet}
\caption{DehazeNet网络结构}
\end{figure}

\subsection{特征提取层\quad}
特征提取层通过卷积操作和非线性激活函数实现。卷积操作使用16个大小为$5 \times 5 \times 3$的卷积核，不同于一般使用ReLU或tanh激活函数，这一层使用Maxout\cite{ref19}激活函数，并设置其参数为$k = 4$。以下使用一个简单的例子讲解Maxout激活函数。
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Maxout}
\caption{传统激活函数与Maxout激活函数}
\end{figure}
上图中，(a)表示传统的激活函数，(b)表示Maxout激活函数。(a)中，我们有$$Y = f(W \otimes X + b)$$
其中，$f$为常用的激活函数，如tanh，ReLU，sigmoid，参数个数为 $2 + 1 = 3$个。(b)中，我们有
\begin{equation}
Y = \max\{W_1 \otimes X + b_1, W_2 \otimes X + b_2, W_3 \otimes X + b_3, W_4 \otimes X + b_4, W_5 \otimes X + b_5\}
\end{equation}
其中参数个数有 $(2 + 1) \times 4 = 12$个。由此知，Maxout单元会成倍的增加参数个数，成倍因子为Maxout单元的参数$k$。

由式(3.3)可知，Maxout激活单元是分段的线性函数，因而可以拟合任意的凸函数，如图3.6所示。
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Maxout-fitting}
\caption{Maxout激活单元}
\end{figure}

\subsection{多尺度映射\quad}
多尺度特征在图像去雾上被证明是较为有效的，于是本网络也采用了多尺度卷积。具体而言，三个尺度的卷积核大小分别为$3\times 3$， $5\times 5$， $7\times 7$，每个尺度均选用16个卷积核。由于上一层输出特征组中有四张特征图，因此每个卷积核的深度均为4。

\subsection{局部极值\quad}
局部极值采用最大池化，窗口大小为$7\times 7$。上一层输出特征组有$16 \times 3 = 48$张特征图，大小均为$12 \times 12$。因此本层局部极值的输出大小为$48 \times 6 \times 6$。

\subsection{非线性回归\quad}
本层在卷积操作之后采用双边线性整流函数(BReLU)作为激活函数。一方面，sigmoid激活函数容易导致梯度消失，进而导致较慢的收敛速度和不理想的局部极值；另一方面，ReLU激活函数比较适用于分类问题，不是非常适合回归问题，如这里的图像复原。受sigmoid函数和ReLU函数启发，DehazeNet提出使用BReLU激活函数，该函数可以保持双边约束以及局部线性。下图为BReLU和ReLU激活函数图像对比，在本方法中，$t_{min}$和$t_{max}$分别取值为0和1。

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{BReLU}
\caption{ReLU和BReLU激活单元对比}
\end{figure}
本层仅使用一个卷积核，其大小为$6 \times 6$，因而输出张量大小为$1 \times 1$，经过BReLU激活函数之后输出一个0-1之间的标量值，即为该图片块的透射率值。

\subsection{网络参数\quad}
表3.1显示了DehazeNet中各层的参数设置。在本网络中，卷积层的偏置恒为0，只设置权重参数，该网络中可训练参数共有$16 \times 4 \times 5 \times 5 + 16 \times 4 \times (3 \times 3 + 5 \times 5 + 7 \times 7) + 48 \times 6 \times 6 = 8240$个。

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{DehazeNet参数设置}
    \begin{tabular}{c|c|c|c|c|c}
    \hline
    层     & 类型    & 输入尺寸  & 个数    & 卷积核大小 & 补0 \\
    \hline
    \multirow{2}[1]{*}{特征提取} & 卷积    & $3 \times 16 \times 16$ & 16    & $5 \times 5$ & 0 \\
          & Maxout & $16 \times 12 \times 12$ & 4     & —     & 0 \\
	\hline
    \multirow{3}[0]{*}{多尺度映射} & \multirow{3}[0]{*}{卷积} & \multirow{3}[0]{*}{$4 \times 12 \times 12$} & 16    & $3 \times 3$ & 1 \\
          &       &       & 16    & $5 \times 5$ & 2 \\
          &       &       & 16    & $7 \times 7$ & 3 \\
	\hline
    局部极值  & 最大池化  & $48 \times 12 \times 12$ & —     & $7 \times 7$ & 0 \\
	\hline
    \multirow{2}[1]{*}{非线性回归} & 卷积    & $48 \times 6 \times 6$ & 1     & $6 \times 6$ & 0 \\
          & BReLU & $1 \times 1$ & 1     & —     & 0 \\
    \hline
    \end{tabular}%
  \label{tab:3.1}%
\end{table}%

\subsection{无雾图像计算\quad}
如前所述，DehazeNet的输入是$16 \times 16 \times 3$的图片块，输出该图片块的透射率值。在网络训练好之后，对含雾图像去雾时，需要先划分出图片块，然后预测每个图片块的透射率值从而得到整张图片的透射图，之后使用引导滤波的方法对该透射图进行优化。估计大气光时，需先选出透射图中前百分之0.1像素强度值的像素点位置，并在这些位置中选取最高像素值作为大气光值。最后按照大气散射模型采用下式计算无雾图像
\begin{equation}
J(x) = \frac{I(x) - A(1 - t(x))}{t(x)}
\end{equation}

\section{MSCNN\quad}
与DehazeNet类似，Ren\cite{ref17}等人提出了多尺度卷积神经网络(MSCNN)以预测透射图。该网络结构如图3.8所示。由图可知，该网络分为粗尺度网络和细尺度网络，粗尺度网络的输出作为细尺度网络的一个输入，该网络包含有卷积层、池化层、上采样层、线性组合层以及融合层。

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{MSCNN}
\caption{MSCNN网络结构}
\end{figure}

\subsection{网络各层介绍\quad}
	输入层为彩色含雾图片，具有三个通道。
\par 卷积层采用线性整流函数(ReLU)作为激活函数，并且含有偏置。
\par 池化层至于卷积层之后，采用的下采样因子为2，也即窗口大小为$2 \times 2$。
\par 上采样层采用$2 \times 2$的窗口。
\par 线性组合层旨在融合最后一个上采样层的特征，并采用sigmoid激活函数。具体而言，该层将上采样层输出的特征图进行线性组合，并添加偏置，之后应用sigmoid激活函数。
\par 细尺度网络将粗尺度网络的输出和第一个上采样层的输出融合，共同作为第二个卷积层的输入。

\subsection{网络参数\quad}
	本网络在训练时，需先训练粗尺度网络，后训练细尺度网络。粗尺度网络训练时，可训练参数个数为$(5 \times 3 \times 11 \times 11 + 5) + (5 \times 5 \times 9 \times 9 + 5) + (10 \times 5 \times 7 \times 7 + 10) + (10 + 1) = 6321$个，其中包含权重和偏置；细尺度网络训练时，可训练参数个数为$(4 \times 3 \times 7 \times 7 + 4) + (5 \times 5 \times 5 \times 5 + 5) + (10 \times 5 \times 3 \times 3 + 10) + (10 + 1) = 1693$个，包含权重和偏置。

\subsection{无雾图像计算\quad}
	由该网络结构可知，输出透射图与输入RGB图像具有同样的高度和宽度，因此无需对输入图像进行划分。在使用本网络进行无雾图像计算时，大气光值的估计与DehazeNet中估计方法一致，并按照下式进行无雾图像的计算
\begin{equation}
{\bf J}(x) = \frac{{\bf I}(x) - {\bf A}}{\max\{0.1, t(x)\}} + {\bf A}
\end{equation}

\section{AOD-Net\quad}
上述两种基于神经网络的方法都采用了同一种思路，即先估计透射图，后根据大气散射模型对图片去雾。这种思路存在两方面的问题，其一，这种思路并不是直接最小化去雾生成图片和无雾图片的差异，因而生成的去雾图片会与真实无雾图片存在较大偏差；其二，分别估计透射率和大气光值产生的误差会累积，进而加剧估计偏差。由此，Li\cite{ref18}等人提出了AOD-Net，完全将去雾转化为一个端对端的问题，不需要任何的中间步骤估计参数，如大气光值。

\subsection{问题重构\quad}
根据大气散射模型，无雾图片可以由以下方程计算得到
\begin{equation}
J(x) = \frac{1}{t(x)}I(x) - A\frac{1}{t(x)} + A
\end{equation}
为了不用分开估计透射图$t$和大气光值$A$，Li等人将式(23)重写为下式
\begin{equation}
\begin{aligned}
J(x) = K(x)I(x) - K(x) + b \\
K(x) = \frac{\frac{1}{t(x)}(I(x) - A) + A - b}{I(x) - 1}
\end{aligned}
\end{equation}
其中$b$为一个常数偏置，默认值为1。经过上述转换，我们便无需分开估计$t$和$A$，而是最小化上式输出$J(x)$和真实无雾图之间的误差。

\subsection{网络结构\quad}

如图3.9所示，该网络结构分为两个模块，一个模块用来估计$K(x)$，另一个模块用来逐元素计算生成无雾图片。前一个模块为全卷积网络，采用五个卷积层和三个融合层。其中，每一个卷积层都只使用三个卷积核，网络中使用不同大小的卷积核以获得多尺度特征，而使用融合层则用以弥补在卷积操作中丢失的信息。不同于DehazeNet，本网络中每个卷积层之后均使用ReLU激活函数。该网络的输入为含雾图片，输出为生成的无雾图片，训练以及预测未知含雾图片均比较简便。
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{AOD-Net}
\caption{AOD-Net网络结构}
\end{figure}

\section{无雾图像计算\quad}
{\Large 扯也要扯点内容}

\section{本章小结\quad}
	由上可知，基于神经网络的去雾算法有更大的灵活度，网络结构的选择层出不穷，但我们也应注意到，以上介绍的三种算法均依据大气散射模型进行去雾，是否存在不基于大气散射模型、端对端、高效快速的去雾算法，这不得而知。本论文中，我们着重研究各种方法的去雾效果，以及他们的泛化能力，并将这些方法应用到雾天视频以观察其实际效果。

\chapter{算法实现及评估\quad}

本章我们具体讲解每种算法的实现，以及他们在统一测试数据上的表现。注意到，前面介绍的几种方法中，只有暗通道法是直接对含雾图像进行处理，不需要其对应的无雾图像，而另外几种方法除含雾图像外，均需要其他信息，如该图像对应无雾图像，或该图像对应透射图，或该图像中图片块对应的透射率值。在实际中，我们几乎不可能找到大量的含雾—无雾图片对，而任意给定一张含雾图像，我们也不知道该图像的透射率或投射图，因此，现存文献中均使用图片合成的方法对上述神经网络进行训练。

含雾图片的合成基于两个假设\cite{ref20}：1) 图片内容与景物深度或介质透射率无关，也即同一图片内容可以出现在不同图片的不同深度；2) 局部范围内，景物深度相近，也即在一个较小的窗口里，该窗口中所含像素点对应的景物深度值相近。依据上述两条假设，我们可以采用下述方法生成含雾图片块：给定无雾图片块$p_j$，以及大气光值$A$，我们随机生成透射率值$t\in [0,1]$，根据${\bf p}_I = t{\bf p}_j + (1 - t){\bf A}$计算含雾图片块。同时，为减少参数学习的不确定性，设置大气光值为$A = [1,1,1]$。

以上生成含雾图片的方法影响了许多后续研究，这让大规模含雾—无雾图片对的获取成为可能，是一项开创性的工作。在此基础上，Li\cite{ref21}等人提出了RESIDE\footnote{\url{https://sites.google.com/view/reside-dehaze-datasets}.}数据集—一个去雾算法学习评估的基准数据集，下面会介绍。

此外，由第二条假设可知，一个较小的图片块具有相同的透射率值，我们可以通过图片块预估该图片块中所有像素点的透射率值，此即DehazeNet所采用的方法。

\section{RESIDE数据集\quad}
RESIDE数据集旨在解决去雾领域含雾—无雾图片对难以获取的困境，通过大规模合成室内、室外含雾图片，为研究人员提供一个综合全面的去雾数据集。此外，该数据集提供了一个评价去雾算法的平台，由学者提出的去雾算法可以使用该数据集的数据进行训练、评估，从而界定各种算法的效果。评估算法时，该平台提出了一系列的评价指标，包括但不限于峰值信噪比PSNR、结构相似性SSIM，以及人眼评价去雾效果。

根据公式(1.1)，(1.2)可得出下式
\begin{equation}
I(x) = J(x) e^{-\beta d(x)} + A(1 - e^{-\beta d(x)})
\end{equation}
由上式可知，含雾图片的生成需要无雾图片$J$，该图片对应的深度数据$d$，大气光值$A$，大气散射系数$\beta$。RESIDE数据集的生成使用了NYU2\cite{ref22}和Middlebury stereo\cite{ref23}深度数据集，并从该数据集获取了对应的无雾图片，因而获取了$J$和$d$的信息。在此基础上，Li等人在$[0.7, 1.0]$之间对大气光值$A$随机取值，在$[0.6, 1.8]$之间对$\beta$随机取值，从而生成含雾图片。

在该数据集网站上可以看到，RESIDE数据集含有五个共训练测试的子集，分别为：室内训练集ITS，共110000张含雾图片；室外训练集OTS，共313950张含雾图片；客观合成测试集SOTS，共1000张含雾图片；应用驱动测试集RTTS，共4322张；综合主观测试集20张。该数据集数据量之大，图片内容之全，以及评价指标之多，均为领域之最。

\section{算法实现与参数设置\quad}
本节我们讲解各算法的具体实现及其参数设置。对于部分算法，作者在提出算法时已经给出参数值，我们不做更改。而另外一部分算法，作者并未给出全部参数的参考值，因此我们自行调优给出。

\subsection{暗通道法 (DCP)\quad}
	暗通道法是直接对含雾图片进行处理的算法，不需要对应的无雾图片，也不需要对图片大小进行修改，算法输入是任意的含雾图片。算法具体实现步骤为：
\begin{enumerate}
\item 根据公式(2.1)计算图像暗通道
\item 根据公式(2.8)计算透射图
\item 根据引导滤波算法优化透射图
\item 根据2.4节提出方法估计大气光值
\item 根据公式(2.15)计算无雾图像
\end{enumerate}

其中，计算暗通道时需给定窗口大小，我们设置该大小为$15\times 15$。引导滤波中使用盒子滤波的半径值设为40，正则化参数设为0.001。

\subsection{DehazeNet\quad}
如前所述，DehazeNet输入数据为$16 \times 16 \times 3$的图片块，输出为该图片块的透射率值。我们采用RESIDE数据集的ITS子集作为训练数据。值得注意的是，ITS中给出的数据是含雾图片和该图片对应的投射图，因此我们需要对该图片进行划分。以一张$320 \times 240$的彩色图片及其对应投射图为例，我们用$16 \times 16$的窗口在该图片及其对应投射图上不重复随机采样，将该采样图片块作为一个样本。由于样本标签是该采样图片块对应透射率值，而我们采样出投射图的图片块是一个$16 \times 16$的数值矩阵，因此我们对投射图图片块取均值作为样本标签。经此处理，一张$320 \times 240$的彩色图片可以生成300个无关联样本（因为我们采用不重复随机采样）。由于训练样本数据大小为$16 \times 16 \times 3$，与ITS数据集中图片大小无关，因此我们不需要统一图片大小尺寸，将图片高度和宽度均调整为距自身最近的16的整数倍即可。原作者训练网络时使用了100 000个样本，本论文一方面扩大训练数据量，另一方面考虑到机器性能以及运行时间，最终决定在ITS数据集中随机抽取10 000张图片生成样本进行训练。
	
DehazeNet使用均方误差作为损失函数
\begin{equation}
L(\Theta) = \frac{1}{N}\sum_{i = 1}^N {||F(I_i^P; \Theta) - t_i||}^2
\end{equation}
其中，$F(I_i^P; \Theta)$为网络输出，$t$为实际透射率值，$N$为使用图片块个数。本网络使用随机梯度下降优化器，动量设置为0.9，衰减率设置为$5e-4$，学习率初始化为0.005，每训练10轮让学习率减半，共训练50轮。权重初始值从均值为0，标准差为0.001的正态分布中随机取值，偏置恒设置为0，共8240个参数。训练过程中，我们按4:1的比例划分训练集和交叉验证集，并将batch size设为100。

训练好DehazeNet之后，我们采用划分图片块的方法对测试图片进行预测，并使用引导滤波优化投射图，之后如前所述估计大气光值并计算无雾图像。

\subsection{MSCNN\quad}
MSCNN的输入为含雾图像，输出为该图像对应投射图。我们使用RESIDE数据集中的ITS子集对该网络进行训练。原作者训练时使用了6 000张无雾图片并生成18 000个样本作为训练集，本论文采用更大的数据量并考虑到机器性能和处理时间，最终从ITS数据集中随机抽取30 000张图片用作训练。此外，该网络在训练时将所有图片统一尺寸为$320 \times 240$。本论文采取类似做法。

MSCNN按下式计算损失函数
\begin{equation}
L(t_i(x), t_i^{\ast}(x)) = \frac{1}{q}\sum_{i = 1}^q {||t_i(x) - t_i^{\ast}(x)||}^2
\end{equation}
其中，$q$为含雾图片张数，$t_i(x)$为网络生成投射图，$t_i^{\ast}(x)$为实际使用的投射图。值得注意的是，本网络中粗尺度网络和细尺度网络是分开训练的，两次训练均采用此损失函数。网络在训练时，先训练好粗尺度网络，而后将粗尺度网络的输出作为细尺度网络的一张特征图，继续训练细尺度网络。本网络使用随机梯度下降优化器，初始学习率设置为0.001，动量设置为0.9，衰减率设置为$5e-4$，每十轮让学习率减至原值的10\%，共训练四十轮。训练过程中，我们按4:1的比例划分训练集和交叉验证集，并设置batch size为100。

训练好MSCNN之后，我们用之对测试图片进行预测，并按照传统方法估计大气光值，进而计算无雾图像。MSCNN对传入图片的限制是该图片的高度值和宽度值均为偶数，基于此，我们将测试图片大小调整为距自身最近的偶数值，以尽量保存测试图片信息。

\subsection{AOD-Net\quad}
AOD-Net的输入为含雾图片，输出为去雾图片。我们使用RESIDE数据集中的OTS子集对该网络进行训练。原作者训练时使用27 256个样本，我们采用更大的数据量并考虑机器性能和处理时间，最终从OTS数据集中随机选取40 000张图片用于训练。AOD-Net是一个全卷积网络，因此对输入图片大小没有限制，但是考虑到我们训练时采取分批次的方法，而每一批次的数据应当具有相同大小，于是我们将所有训练数据统一尺寸为$320 \times 240$。训练过程中，我们按4:1的比例划分训练集和交叉验证集，并设置batch size为32。
	
AOD-Net使用MSE作为损失函数，使用随机梯度下降优化器，其中动量设置为0.9，衰减率设置为0.0001，并且将梯度值限制在$[-0.1, 0.1]$之间。本网络采用高斯分布随机变量初始化权重。
	
网络训练好之后，即可直接对测试图片进行去雾。由于该网络是全卷积网络，因此不用调整图片大小。

\section{图像质量评价指标\quad}
图片在获取、处理、压缩、储存或传输时都有可能引入噪声，本论文研究的雾属于噪声的一种。本论文对图片的处理最终是以人为受众的，因此处理效果由由受众主观评价最为准确。然而，主观评价需要花费大量的时间精力，且评价结果极易收到评价者的影响（如同一评价者在不同的环境下有可能对同一张图片给出不同的评价结果），因而主观评价是不稳定的。

由此，学者们展开了客观评价指标的研究。图像质量评估（Image Quality Assessment, IQA）中的客观指标可以分为三类[24]：全参考(Full-Reference, FR)指标，即可以获得真实图片时对处理图片进行评价的指标；部分参考(Reduced-Reference, RR)指标，旨在能获取部分真实图片信息时对处理后图片进行评价；无参考(No-Reference, NR)指标，即无法获取真实图片时进行评价的指标。NR-IQA方法通常分为两类，一类是单一用途质量评估方法，这类方法需要先知道降质类型，如噪声、模糊等；另一类是基于训练和学习的通用质量评估方法，此类方法先自动对图片所含噪声类型进行判断分类，之后针对降质类型计算评价指标。此类方法不受限于某一特定的图片降质原因，因而具有更广的应用前景。除传统方法之外，也有学者应用深度学习进行图像质量评价，神经网络有复杂的结构和强大的非线性拟合能力，因而性能会超过传统方法，但该方法需要大量的数据进行训练，且网络设计和训练极度强调技巧，本文暂不深入研究。本论文中，我们采用全参考指标PSNR，SSIM和通用型的无参考指标SSEQ，BRISQUE对各算法去雾效果进行评价。下面对各指标逐一介绍。



\subsection{峰值信噪比PSNR\quad}

在通信领域，信噪比是指信号功率与噪声功率之比，峰值信噪比表示信号最大可能功率与噪声功率之比值。在图像处理领域，峰值信噪比则通过均方误差MSE进行定义，即像素点最大像素值与MSE的比值。

假设有一张去雾图像$I$和其对应的无雾真实图像$J$，其大小为$m \times n \times 3$，其中3是颜色通道个数。则均方误差定义为
\begin{equation}
MSE = \frac{1}{3mn}\sum_{i = 0}^{m - 1}\sum_{j = 0}^{n - 1}{[I(i, j) - J(i, j)] }^2
\end{equation}
峰值信噪比为
\begin{equation}
PSNR = 10 log_{10}(\frac{MAX_I^2}{MSE})
\end{equation}
其中，$MAX_I$表示图像点像素最大值。由上可知，峰值信噪比越大，图像失真越小，去雾效果越好，PSNR指标值通常在0-100之间。
	
MSE和PSNR的计算非常简便，也具有清晰的物理含义，因此被广泛应用于重建图像的评估，但这两个指标对图像的的评价结果与人眼的评价结果不完全一致，因此存在应用的局限性。

\subsection{结构相似性SSIM\quad}
与PSNR不一样，SSIM更强调与评价指标与人眼判断具有较好的一致性。人眼在观看图片时，会潜意识将图片内容结构化，也即认为相邻像素之间是存在关联的，因此Wang\cite{ref24}等人在设计SSIM时着重考虑了图片间的结构性信息。
	
Wang等人设计的SSIM系统将图像相似性的测量分为亮度、对比度以及结构的测量，按照下式进行SSIM的计算
\begin{equation}
SSIM(x, y) = {[l{\bf (x, y)}]}^{\alpha}\cdotp {[c{\bf (x, y)}]}^{\beta}\cdotp {[s{\bf (x, y)}]}^{\gamma} 
\end{equation}
其中
\begin{equation}
l{\bf (x, y)} = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}
\end{equation}
\begin{equation}
c{\bf (x, y)} = \frac{2\sigma_x\sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2}
\end{equation}
\begin{equation}
s{\bf (x, y)} = \frac{\sigma_{xy}+ C_3}{\sigma_x\sigma_y + C_3}
\end{equation}

$l{\bf (x, y)}$比较$x$和$y$的亮度，$c{\bf (x, y)}$比较$x$和$y$的对比度，$s{\bf (x, y)}$比较$x$和$y$的结构，$\alpha > 0, \beta > 0, \gamma > 0$为调整三者相对重要性的参数，$C_1, C_2, C_3$为维持三者稳定性的常数，$\mu_x, \sigma_x, \mu_y, \sigma_y$分别为$x$和$y$的平均值和标准差，$\sigma_{xy}$为$x$和$y$的协方差。

SSIM指标值在0-1之间，用SSIM评估图像相似度时，计算的SSIM越大，则两张图片相似度越高。在实际使用时，为简化上述表达式，一般令$\alpha = \beta = \gamma = 1$并使$C_3 = 3/2 C_2$，得
\begin{equation}
SSIM({\bf x, y}) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy}+ C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}
此外，由于视觉的局部性，我们使用固定大小的窗口在图片上滑动，计算每个窗口的SSIM值，并使用高斯加权函数对每个窗口值进行加权平均以防止出现块状现象。

\subsection{SSEQ\quad}
SSEQ\cite{ref25}(Spatial-Spectral Entropy-based Quality)采用位置熵和谱熵来表征图片的结构性及像素之间的相关性。SSEQ方法首先计算图像中的位置熵特征以及谱熵特征，之后采用一个两步框架来分辨降质类型并评估图片质量。其中，SSEQ方法使用支持向量机进行降质类型预测，使用支持向量回归用于图片质量评估。本方法由Liu等人实现并公开。SSEQ指标值通常在0-100之间，0表示图片质量最高。

\subsection{BRISQUE\quad}
To do here\cite{ref26}.

\section{算法评估\quad}
本节我们对上述介绍的算法进行评估。我们从SOTS数据集中indoor、outdoor子集各随机采样50张共100张含雾图片及其对应无雾图片对各算法进行测试，分析对比由算法计算出的各指标，之后观察各算法在实拍图片上的效果，计算指标。最后，我们将各算法应用于行车记录仪拍摄视频，以观察应用效果。

本论文中，所有算法均用python/keras实现，在同一台Linux机器上运行，机器配置为Intel(R) Core(TM) i7-6700k 4GHz, 24GB RAM，Nvidia GeForce GTX 1080 Ti GPU, 8G显存。

\subsection{基于合成图片的评价\quad}
我们首先比较DCP、DehazeNet、MSCNN、AOD-Net在SOTS数据集上的去雾效果，评价指标为PSNR、SSIM、SSEQ、BRISQUE。各算法得分如表4.1所示、

\begin{table}[htbp]
  \centering
  \caption{合成图片测试算法得分表}
    \begin{tabular}{c|c|c|c|c}
    \hline
         & DCP    & DehazeNet  & MSCNN    & AOD-Net  \\
    \hline
    	PSNR	 & 17.73    & 13.62 & 16.14    & 19.55 \\
	\hline
     SSIM     & 0.85 & 0.75 & 0.72     & 0.85   \\
	\hline
    SSEQ &   &  &     &  \\
    \hline
     BRISQUE     &       &       &   & \\
     \hline
  
    \end{tabular}%
  \label{tab:4.1}%
\end{table}%
从表4.1可以看出，就全参考指标而言，AOD-Net的得分较高，尤其是PSNR。这一结果与我们的预期是相符的。在PSNR的定义中可以看见，两张图片间的均方误差(MSE)越小，则计算得到的PSNR值越大，得分越高。而AOD-Net在训练时即是以减小图片间的MSE为目标的，因此该网络取得PSNR最高分是意料之中的。同时我们也注意到，虽然DehazeNet和MSCNN训练时都是最小化预测透射率和实际透射率之间的误差，但MSCNN是直接最小化两张透射图之间的误差，因而预测得到的透射图与实际投射图更接近，由此计算出的去雾图片与原图误差更小，所以MSCNN相比DehazeNet具有更高的PSNR得分。同理，DehazeNet比MSCNN更强调图片的内部结构，采用了划分图片块的方法训练网络并对含雾图片去雾，因此DehazeNet比MSCNN取得更高的结构相似性得分是合理的。除此之外，我们应该注意到，在本测试集上，暗通道法取得了非常显著的效果，全面超过DehazeNet和MSCNN。我们检查测试集发现，从SOTS中随机采样得到的图片中，含天空区域的图片较少，而DCP非常适用于不含天空区域或者不含与天空具有类似颜色物体的图片的去雾，这解释了为什么在这一测试集上该方法能取得如此优越的效果。

就无参考指标而言，to do here. 全参考指标和无参考指标对去雾算法评价的不一致，也表明了目前研究的困境，即现行的客观图像质量评估方法不能很好地适用于去雾图像，NR-IQA仍是一个非常困难的问题。

图4.1（图片左边两张，右边四张）展示了各算法在不含天空区域图片上的去雾表现，可以看见 to do here。图4.2展示了各算法在含天空区域图片上的去雾效果，可以看见，DCP方法在去雾时会对天空区域造成较大失真，而其他方法to do here.

为了进一步研究暗通道法的效果及其对天空区域的去雾效果，我们对He提出的暗通道法进行微调，我们设置天空区域像素最大值为220，设置透射率最小值为0.2，将此微调后的暗通道法记为DCP-2.图4.3（这里用实际图片就行）展示了DCP和DCP-2的去雾效果对比图。两者对比我们发现，调整后的暗通道法对天空区域处理更接近实际情况，而去雾效果也比原方法好。对此，我们深入研究发现，原方法估计的大气光值往往较高，有的甚至达到了255，也即像素最高值，对应纯白色。但现实生活中，天空并不是纯白色，即便在浓雾天气其像素值也不会达到255，因此我们微调后，天空区域的计算更接近实际值，也更符合人的视觉系统。同时，原方法中设置透射率最小值为0.1，但未给出具体原因，因此我们有理由认为最小透射率设置为0.1时保留的景物信息太少而导致去雾效果不理想。适当上调透射率最小阈值后，图片的去雾效果更好，这也证实了我们的猜想。

另外，我们发现，各算法在浓雾环境下的去雾效果均不理想，因为浓雾情况下，图片中含有的景物信息过少，而去雾是一个图像复原的过程，我们没有足够的信息来复原无雾图像，如图4.4所示。

\subsection{基于实际图片的评价\quad}
我们通过实地拍摄及在RTTS数据集中随机采样，共选取了100张实际的含雾图片。图4.5，4.6显示了四种算法在实际图片上的去雾效果。可以看见to do here.

此外，表4.2展示了各算法在此数据集上的SSEQ、BRISQUE指标。可见to do here.
\begin{table}[htbp]
  \centering
  \caption{实际图片测试算法得分表}
    \begin{tabular}{c|c|c|c|c}
    \hline
         & DCP    & DehazeNet  & MSCNN    & AOD-Net  \\
    \hline
    SSEQ &   &  &     &  \\
    \hline
     BRISQUE     &       &       &   & \\
     \hline
  
    \end{tabular}%
  \label{tab:4.2}%
\end{table}%

\subsection{去雾速率\quad}
表4.3展示了各算法对一张$480 \times 640$大小的图片去雾所花费的时间，所有算法均在同一机器上使用python实现，因此具有可比性。由表可知，AOD-Net和MSCNN的计算速率非常快，大幅高于DCP和DehazeNet。这一结果是可以预见的。DCP和DehazeNet均需使用引导滤波对估计得到的透射图进行优化，而引导滤波的计算相比于神经网络的前向传播是非常缓慢的。训练好的MSCNN和AOD-Net在对一张图片去雾时，仅需进行一次前向传播，之后不需对透射图进行优化，这一过程非常迅速。除此之外，MSCNN的网络结构更为复杂，且估计出透射图后仍需按传统方法估计大气光并计算得到去雾图像，而AOD-Net网络结构简单，对图像进行端对端的处理，直接前向传播即可计算出去雾图像，因此AOD-Net具有更快的运行速率。在实际应用时，我们要求算法能够快速乃至实时对获取图片进行去雾，因此传统方法和DehazeNet计算速率不满足要求。

\begin{table}[htbp]
\centering
\caption{各算法去雾速率对比 (单位: s)}
\begin{tabular}{c|c|c|c|c}
\hline
图片大小 & DCP & DehazeNet	& MSCNN	& AOD-Net \\
\hline
$480\times  640$ & 21.99 & 20.63 & 0.05 & 0.02\\
\hline
\end{tabular}
\label{tab: 4.3}
\end{table}

\subsection{视频去雾评价\quad}
自动驾驶巨头特斯拉CEO马斯克及其团队曾说激光雷达在自动驾驶领域已经走到尽头，人们在驾驶的时候就是只凭视觉的，所以马斯克团队说未来的自动驾驶也应该只靠视觉。将去雾算法应用到自动驾驶车辆，则该车辆行驶时能获取的信息增多，自动驾驶的应用面会更广泛，其可靠性也会上升。

考虑到AOD-Net极高的计算速率，我们应用其对视频文件进行去雾。雾天视频来源于YouTube\cite{ref27}。图4.7展示了算法对该视频的去雾效果。由此可知to do here具有一定的应用前景。

\section{本章小结\quad}
本章我们详细介绍了RESIDE数据集中含雾图片的合成以及该数据集的组成，而后讲解了我们在研究过程中对DCP、DehazeNet、MSCNN、AOD-Net的具体实现及参数设置。最后，我们具体介绍了图像质量评估领域的全参考指标PSNR、SSIM和无参考指标SSEQ、BRISQUE，并从合成图片去雾评价、实际图片去雾评价、去雾速率对比以及视频处理去雾评价四个方面对各算法进行了评估。综合而言，我们认为基于神经网络的去雾算法具有更强的去雾能力和更广阔的应用前景。

\chapter{总结与展望\quad}
\section{总结\quad}
本文旨在探索比对去雾领域的前沿算法。以表征含雾图片生成的大气散射模型引入，本文介绍了研究背景与意义，并综述本领域的发展变迁。以暗通道法为例，本文介绍了基于先验知识的去雾算法。鉴于卷积神经网络在图像处理中的卓越效果，本文介绍了卷积神经网络及其典型层，并详细介绍基于卷积神经网络的去雾算法DehazeNet，MSCNN和AOD-Net。

神经网络的训练需要大量数据，尤其是含雾—无雾图片对。由此，本文介绍了领域中基准性数据集RESIDE。此外，我们具体讲解了各算法实现及其参数设置。本文采用全参考指标PSNR，SSIM和无参考指标SSEQ、BRISQUE对图像质量进行评价，我们发现。。。我们也将各算法应用于实际图片，发现。。。考虑到去雾算法的应用价值，我们将各算法应用于雾天视频，并得出。。。同时，我们发现各算法速率。。。
	
\section{展望\quad}
图像去雾化是图像处理中较为热门的领域。许多学者致力于提出高效、快速、通用的去雾方法，但由于单幅图像去雾是一个病态的问题，这一通用方法仍未被发现。研究过程中，我们注意到，现行的去雾方法都是基于大气散射模型的，也即我们是按照给出$I$求解$J$的思路进行去雾的。但我们实验发现，从合成图像上训练出的去雾模型在实际图像中表现并不好，说明通过这一模型合成出的含雾图像与实际的含雾图像存在较大差距。是否存在一个更为合理的描述含雾图片生成的模型，我们不得而知。

此外，神经网络模型在训练时遇到的最大困难，便是样本的获取。实际生活中，我们几乎不可能获取到含雾—无雾图片对，因此极大的限制了神经网络模型的泛化能力。如果能获取到真实的含雾—无雾图像对，神经网络的优越性便会充分体现。在图像质量评价时，我们意识到现行的IQA方法不能很好的适用于去雾图片的质量评价，全参考指标和无参考指标得出的结论并不一致，我们认为可以应用深度学习的方法对去雾效果进行评价。

从本文的研究可以知道，图像去雾可以应用到行车记录仪，更好的记录路面交通信息，也为将来完全基于视觉的自动驾驶系统做铺垫，还可以应用到交叉口视频监控，更清楚地捕捉交叉口的交通行为，更好地服务于交通管控。













\begin{Acknowledgement}{}
\par To my family, friends, and tearchers.
\par 从家庭中获得了最多的养分，也对家庭亏欠得最多。Luckily, I have a whole life to pay you back, and I will pay you back.
\par 饺子姐，how you doing? 你是对我的大学乃至以后的人生都有关键影响的人，大二大三两年如果没有你，我可能会步入温水，become nobody. You are the first one that pops up in my mind when I recall my undergraduate years.
\par 感谢远在长春的基友，感谢你陪我扯犊子，谈天论地，挥斥方遒，指点江山。我乐于领略你的讲话精神，享受互损互吹，you are my shaper.
\par 感谢小魏同学，you are a dear friend of mine, always.
\par 感谢富婆瑾神，情圣阿曾，天才凡仔仔，you brought me laughter. You stood with me when I broke down.
\par 感谢建安和文强，你们教会了我太多太多。I don't know what will become of me if I confined myself to this tiny world, but I know I'll hate him badly.
\par 感谢Sunny, you are my drive and my comfort. You are the goodness I'd love to think about when I am down.
\par 感谢Martin \& Soffia, your kindness will stay with me in my life to come.
\par 感谢Janet \& Kaitlyn, I could never come here without your support.
\par 感谢刘老师，带我进行科研训练，让我受益良多。感谢李老师，细致的指导让我体会到何为学者风范。感谢罗老师，从工作、生活、人生各方面指引我。You are remembered.
\par 感谢何老师，为我提供悉心的毕设资源和指导，让我能够安心地完成毕业设计。感谢吕学长和江学长，在我毕设过程中不嫌烦扰的为我答疑解惑。 
\par Finally, thanks to those who forced me to realize the cruelty of life. I am still alive, so I guess I am stronger :).

\end{Acknowledgement}

%\begin{References}
%\end{References}

\begin{thebibliography}{99}  
\setlength{\itemsep}{0pt}
\bibitem{ref1} He, K., Sun, J., \& Tang, X. (2010). Single image haze removal using dark channel prior. IEEE transactions on pattern analysis and machine intelligence, 33(12), 2341-2353.
\bibitem{ref2} McCartney, E. J. (1976). Optics of the atmosphere: scattering by molecules and particles. New York, John Wiley and Sons, Inc., 1976. 421 p.
\bibitem{ref3} Narasimhan, S. G., \& Nayar, S. K. (2000). Chromatic framework for vision in bad weather. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662) (Vol. 1, pp. 598-605). IEEE.
\bibitem{ref4} Narasimhan, S. G., \& Nayar, S. K. (2002). Vision and the atmosphere. International journal of computer vision, 48(3), 233-254.
\bibitem{ref5} Narasimhan, S. G., \& Nayar, S. K. (2003). Contrast restoration of weather degraded images. IEEE Transactions on Pattern Analysis \& Machine Intelligence, (6), 713-724.
\bibitem{ref6} Nayar, S. K., \& Narasimhan, S. G. (1999). Vision in bad weather. In Proceedings of the Seventh IEEE International Conference on Computer Vision (Vol. 2, pp. 820-827). IEEE.
\bibitem{ref7} Schechner, Y. Y., Narasimhan, S. G., \& Nayar, S. K. (2001, December). Instant dehazing of images using polarization. In CVPR (1) (pp. 325-332).
\bibitem{ref8} Shwartz, S., Namer, E., \& Schechner, Y. Y. (2006). Blind haze separation. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1984-1991). IEEE.
\bibitem{ref9} Kim, J. Y., Kim, L. S., \& Hwang, S. H. (2001). An advanced contrast enhancement using partially overlapped sub-block histogram equalization. IEEE transactions on circuits and systems for video technology, 11(4), 475-484.
\bibitem{ref10} Parthasarathy, S., \& Sankaran, P. (2012, August). A RETINEX based haze removal method. In 2012 IEEE 7th International Conference on Industrial and Information Systems (ICIIS) (pp. 1-6). IEEE.
\bibitem{ref11} Tan, R. T. (2008, June). Visibility in bad weather from a single image. In 2008 IEEE Conference on Computer Vision and Pattern Recognition (pp. 1-8). IEEE.
\bibitem{ref12} Fattal, R. (2008). Single image dehazing. ACM transactions on graphics (TOG), 27(3), 72.
\bibitem{ref13} He, K., Sun, J., \& Tang, X. (2012). Guided image filtering. IEEE transactions on pattern analysis and machine intelligence, 35(6), 1397-1409.
\bibitem{ref14} Zhu, Q., Mai, J., \& Shao, L. (2015). A fast single image haze removal algorithm using color attenuation prior. IEEE transactions on image processing, 24(11), 3522-3533.
\bibitem{ref15} Berman, D., \& Avidan, S. (2016). Non-local image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1674-1682).
\bibitem{ref16} Cai, B., Xu, X., Jia, K., Qing, C., \& Tao, D. (2016). Dehazenet: An end-to-end system for single image haze removal. IEEE Transactions on Image Processing, 25(11), 5187-5198.
\bibitem{ref17} Ren, W., Liu, S., Zhang, H., Pan, J., Cao, X., \& Yang, M. H. (2016, October). Single image dehazing via multi-scale convolutional neural networks. In European conference on computer vision (pp. 154-169). Springer, Cham.
\bibitem{ref18} Li, B., Peng, X., Wang, Z., Xu, J., \& Feng, D. (2017). Aod-net: All-in-one dehazing network. In Proceedings of the IEEE International Conference on Computer Vision (pp. 4770-4778).
\bibitem{ref19} Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., \& Bengio, Y. (2013). Maxout networks. arXiv preprint arXiv:1302.4389.
\bibitem{ref20} Tang, K., Yang, J., \& Wang, J. (2014). Investigating haze-relevant features in a learning framework for image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2995-3000).
\bibitem{ref21} Li, B., Ren, W., Fu, D., Tao, D., Feng, D., Zeng, W., \& Wang, Z. (2019). Benchmarking single-image dehazing and beyond. IEEE Transactions on Image Processing, 28(1), 492-505.
\bibitem{ref22} Silberman, N., Hoiem, D., Kohli, P., \& Fergus, R. (2012, October). Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision (pp. 746-760). Springer, Berlin, Heidelberg.
\bibitem{ref23} Scharstein, D., \& Szeliski, R. (2003, June). High-accuracy stereo depth maps using structured light. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings. (Vol. 1, pp. I-I). IEEE.
\bibitem{ref24} Wang, Z., Bovik, A. C., Sheikh, H. R., \& Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4), 600-612.
\bibitem{ref25} Liu, L., Liu, B., Huang, H., \& Bovik, A. C. (2014). No-reference image quality assessment based on spatial and spectral entropies. Signal Processing: Image Communication, 29(8), 856-863.
\bibitem{ref26} Mittal, A., Moorthy, A. K., \& Bovik, A. C. (2012). No-reference image quality assessment in the spatial domain. IEEE Transactions on Image Processing, 21(12), 4695-4708.
\bibitem{ref27} Fog accident in San Antonio TX. (2017, February 1st). Retrieved May 14, 2019, from: YouTube. 

\end{thebibliography}



\newpage



\bigskip
你好啊啊

{\heiti 你好啊啊，黑体}

{\kaishu 你好啊啊，楷书}

%{\mlu 你好啊啊，细明体}

what the hell is going on 

{\fontsize{50}{60}\selectfont Foo}


给公式、图片、表格、参考文献等等加超链接 it is just a label and a ref

参考文献字体没调好！但在改了字体之后TOC里面的字体也会改！实在不行的办法就把正文里的字体改好，然后手动改.toc文件里面的参考文献字体！这是奇技淫巧

























\end{document}